Training for NER tagging...


Epoch 0 train loss 1.5263
correct 1 prediction 17 golden 3341
test p = 0.0588 r = 0.0003 f1 = 0.0006

Epoch 1 train loss 0.7363
correct 657 prediction 1470 golden 3341
test p = 0.4469 r = 0.1966 f1 = 0.2731

Epoch 2 train loss 0.4971
correct 1310 prediction 2595 golden 3341
test p = 0.5048 r = 0.3921 f1 = 0.4414

Epoch 3 train loss 0.3702
correct 1605 prediction 3086 golden 3341
test p = 0.5201 r = 0.4804 f1 = 0.4995

Epoch 4 train loss 0.2914
correct 1733 prediction 2932 golden 3341
test p = 0.5911 r = 0.5187 f1 = 0.5525

Epoch 5 train loss 0.2329
correct 1948 prediction 3322 golden 3341
test p = 0.5864 r = 0.5831 f1 = 0.5847

Epoch 6 train loss 0.1920
correct 2007 prediction 3302 golden 3341
test p = 0.6078 r = 0.6007 f1 = 0.6042

Epoch 7 train loss 0.1592
correct 2053 prediction 3344 golden 3341
test p = 0.6139 r = 0.6145 f1 = 0.6142

Epoch 8 train loss 0.1343
correct 2087 prediction 3329 golden 3341
test p = 0.6269 r = 0.6247 f1 = 0.6258

Epoch 9 train loss 0.1163
correct 2120 prediction 3453 golden 3341
test p = 0.6140 r = 0.6345 f1 = 0.6241

Epoch 10 train loss 0.1018
correct 2136 prediction 3377 golden 3341
test p = 0.6325 r = 0.6393 f1 = 0.6359

Epoch 11 train loss 0.0896
correct 2151 prediction 3402 golden 3341
test p = 0.6323 r = 0.6438 f1 = 0.6380

Epoch 12 train loss 0.0786
correct 2193 prediction 3534 golden 3341
test p = 0.6205 r = 0.6564 f1 = 0.6380

Epoch 13 train loss 0.0698
correct 2197 prediction 3468 golden 3341
test p = 0.6335 r = 0.6576 f1 = 0.6453

Epoch 14 train loss 0.0614
correct 2181 prediction 3441 golden 3341
test p = 0.6338 r = 0.6528 f1 = 0.6432

Epoch 15 train loss 0.0572
correct 2204 prediction 3495 golden 3341
test p = 0.6306 r = 0.6597 f1 = 0.6448

Epoch 16 train loss 0.0492
correct 2228 prediction 3608 golden 3341
test p = 0.6175 r = 0.6669 f1 = 0.6412

Epoch 17 train loss 0.0452
correct 2230 prediction 3548 golden 3341
test p = 0.6285 r = 0.6675 f1 = 0.6474

Epoch 18 train loss 0.0410
correct 2219 prediction 3558 golden 3341
test p = 0.6237 r = 0.6642 f1 = 0.6433

Epoch 19 train loss 0.0373
correct 2236 prediction 3531 golden 3341
test p = 0.6332 r = 0.6693 f1 = 0.6508

Epoch 20 train loss 0.0348
correct 2224 prediction 3503 golden 3341
test p = 0.6349 r = 0.6657 f1 = 0.6499

Epoch 21 train loss 0.0324
correct 2245 prediction 3557 golden 3341
test p = 0.6311 r = 0.6720 f1 = 0.6509

Epoch 22 train loss 0.0295
correct 2241 prediction 3562 golden 3341
test p = 0.6291 r = 0.6708 f1 = 0.6493

Epoch 23 train loss 0.0275
correct 2242 prediction 3526 golden 3341
test p = 0.6358 r = 0.6711 f1 = 0.6530

Epoch 24 train loss 0.0263
correct 2241 prediction 3545 golden 3341
test p = 0.6322 r = 0.6708 f1 = 0.6509

Epoch 25 train loss 0.0249
correct 2245 prediction 3559 golden 3341
test p = 0.6308 r = 0.6720 f1 = 0.6507

Epoch 26 train loss 0.0230
correct 2233 prediction 3537 golden 3341
test p = 0.6313 r = 0.6684 f1 = 0.6493

Epoch 27 train loss 0.0224
correct 2235 prediction 3524 golden 3341
test p = 0.6342 r = 0.6690 f1 = 0.6511

Epoch 28 train loss 0.0215
correct 2240 prediction 3532 golden 3341
test p = 0.6342 r = 0.6705 f1 = 0.6518

Epoch 29 train loss 0.0209
correct 2240 prediction 3524 golden 3341
test p = 0.6356 r = 0.6705 f1 = 0.6526

Training is finished.
Train loss - [1.5263232774185356, 0.7363166443387869, 0.4970909984165186, 0.370242850842685, 0.29138710473899954, 0.23285719190305326, 0.19199599698793324, 0.15916673417747312, 0.13427897531590033, 0.11626412986265347, 0.10177206258147016, 0.08963738992589028, 0.07862983018544767, 0.06979769581539952, 0.06141074689815333, 0.057210150144266655, 0.049217027190605646, 0.04519793393113864, 0.04104112720777602, 0.03731773150682777, 0.034807130893233465, 0.03238659596447396, 0.029514188825090555, 0.027486393687480803, 0.026346656868613424, 0.024872174685082304, 0.023027002720361595, 0.022396152381115714, 0.021505252795103236, 0.020860730189666065]
Metrics of each epoch
precision - [0.058823529411764705, 0.44693877551020406, 0.5048169556840078, 0.5200907323395982, 0.5910641200545702, 0.5863937387116195, 0.6078134463961236, 0.6139354066985646, 0.6269149894863323, 0.613958876339415, 0.6325140657388214, 0.6322751322751323, 0.6205432937181664, 0.6335063437139562, 0.6338273757628596, 0.630615164520744, 0.6175166297117517, 0.6285231116121759, 0.6236649803260259, 0.6332483715661286, 0.6348843848130175, 0.6311498453753163, 0.62914093206064, 0.6358479863868406, 0.6321579689703808, 0.6307951671817926, 0.6313259824710207, 0.6342224744608399, 0.6342015855039638, 0.6356413166855845]
recall - [0.000299311583358276, 0.1966477102663873, 0.3920981741993415, 0.48039509129003294, 0.5187069739598923, 0.5830589643819216, 0.6007183478000598, 0.6144866806345406, 0.6246632744687219, 0.634540556719545, 0.6393295420532774, 0.6438192158036516, 0.6563903023046992, 0.6575875486381323, 0.6527985633043999, 0.6596827297216402, 0.6668662077222388, 0.6674648308889554, 0.6641724034720143, 0.669260700389105, 0.6656689613888057, 0.6719545046393296, 0.6707572583058964, 0.6710565698892547, 0.6707572583058964, 0.6719545046393296, 0.6683627656390302, 0.6689613888057467, 0.6704579467225381, 0.6704579467225381]
f1 - [0.0005955926146515784, 0.2731240906256496, 0.4413746630727763, 0.49945542243659563, 0.5525267017376057, 0.5847215968782831, 0.6042450699984947, 0.6142109199700823, 0.6257871064467766, 0.624080070650574, 0.6359035427210479, 0.6379949577339463, 0.6379636363636364, 0.6453223674548392, 0.6431731052786789, 0.6448215330602691, 0.6412433443660959, 0.6474089127594717, 0.643281635019568, 0.6507566938300349, 0.6499123319696084, 0.6509133082052769, 0.6492829204693612, 0.6529780107761759, 0.6508858553586988, 0.6507246376811594, 0.6493166618202966, 0.651128914785142, 0.6518259857413066, 0.6525855790240349]
